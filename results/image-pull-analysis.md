# AKS Windows Pod Image Pull Performance Analysis

## 1. Scope

This report analyzes cold and warm start image pull performance for Windows container pods on AKS using an Azure Container Registry (ACR) in close proximity to the cluster.

Data comes from the commented event timelines inside:

- `src/acr-runs-tests/48_2019_acr_pod.yaml`
- `src/acr-runs-tests/48_2019_acr_pod_winiso.yaml`

Tests include both **ACR Standard** and **ACR Premium** tiers to quantify the performance impact of registry tier upgrades.

Focus: Time from Scheduled → Started (or Created where Started not logged) and raw image pull duration.

## 2. Test Images

| Image Tag (as logged) | Notes | Reported Size (bytes) | Approx Size (GiB) | Inflated Size |
|-----------------------|-------|-----------------------|-------------------|---------------|
| `run48-10gb-ltsc2019:latest` | Compressed to ~2.87 GiB | 3,081,157,650 | 2.87 | 9.91 GB |
| `run48-74gb-rand-ltsc2019:latest` | Compressed to ~2.88 GiB | 3,089,505,742 | 2.88 | 7.41 GB |
| `run48-winiso-ltsc2019:latest` | Includes a large ISO file to reduce image compression | 8,727,972,237 | 8.13 | 13.17 GB |

> Note:
> - The image compression is very efficient.
> - It explains the similar `Reported sizes` by the AKS image pull for the 2 `run48-*` images, while their uncompressed sizes are significantly larger:
>     - The added data difference of `2.50 GB` was generated by creating an empty file, that compresses extremely well (see Docker file `.\src\acr-runs-tests\dockerfile-10G-win2019`).
> - This led to creating a larger image that includes the full Windows 11 ISO file (about `4.3 GB` uncompressed) to simulate a more realistic large image with less compression potential (see Docker file `.\src\acr-runs-tests\dockerfile-10G-random-win2019`).

## 3. Cold Start Results Summary

Cold starts measured on freshly provisioned Windows nodes with no prior image cache.

### 3.1 ACR Standard Tier - Smaller (~2.87-2.88 GiB) Images

| Run | Scheduled→Started | Image Pull | Effective Pull Rate (MiB/s) |
|-----|-------------------|-----------|------------------------------|
| 10gb-ltsc2019 | 7m09s | 6m18.626s | ~7.8 MiB/s |
| 74gb-rand-ltsc2019 | 3m35s | 3m8.58s | ~15.7 MiB/s |

Variability suggests node/network contention differences.

### 3.2 ACR Standard Tier - Larger (~8.13 GiB) Image Cold Pulls

| Run | Scheduled→Started/Created | Image Pull Duration | Effective Rate (MiB/s) |
|-----|---------------------------|---------------------|------------------------|
| 2 | 6m06s (→Created) | 6m02.724s | 23.0 |
| 3 | 4m23s (→Created) | 4m18.882s | 32.2 |
| 4 | 6m41s (→Created) | 6m36.444s | 21.0 |
| 5 | 7m44s (→Started) | 6m24.221s | 21.7 |
| 6 | 7m44s (→Started) | 6m46.576s | 20.5 |
| 7 | 4m53s (→Started) | 4m03.756s | 34.2 |
| 8 | 8m16s (→Started) | 6m38.140s | 20.9 |

**ACR Standard Aggregate metrics (8.13 GiB image):**

- Average cold pull time: 350.1 s (≈5m50s)
- Median cold pull time: 362.7 s (≈6m03s)
- Average Scheduled→Started: 436.6 s (≈7m17s)
- Fastest pull: 243.8 s (Run 7)
- Slowest pull: 406.6 s (Run 6)
- Effective throughput range: 20.5-34.2 MiB/s; average ~24.8 MiB/s

### 3.3 ACR Premium Tier - Larger (~8.13 GiB) Image Cold Pulls

| Run | Scheduled→Started/Created | Image Pull Duration | Effective Rate (MiB/s) |
|-----|---------------------------|---------------------|------------------------|
| 1 | 3m39s (→Created) | 3m35.747s | 38.8 |
| 2 | 4m47s (→Started) | 3m46.793s | 36.9 |
| 3 | 3m49s (→Created) | 3m44.092s | 37.2 |

**ACR Premium Aggregate metrics (8.13 GiB image):**

- Average cold pull time: 225.5 s (≈3m46s)
- Average Scheduled→Started: 253.3 s (≈4m13s)
- Effective throughput range: 36.9-38.8 MiB/s; average ~37.6 MiB/s

**ACR Premium vs Standard Improvement:**

- Pull time reduction: **35.6% faster** (5m50s → 3m46s)
- Throughput improvement: **51.6% higher** (24.8 → 37.6 MiB/s)
- End-to-end startup reduction: **42.0% faster** (7m17s → 4m13s)

### 3.4 Observations

- **ACR Premium delivers significant performance gains** with ~36% faster cold pull times and 52% higher throughput than Standard tier.
- Using ACR close to AKS cluster region minimizes network latency; tier choice impacts throughput more. The maximum pull duration for the larger image and Premium tier was ~3m46s vs ~6m46s for Standard, and way more with current Artifactory Container registry.
- Pull duration dominates overall startup (>80% of Scheduled→Started in most runs).
- Variability in Standard tier (20-34 MiB/s) vs consistent Premium performance (37-39 MiB/s) highlights Premium's dedicated throughput.

## 4. Warm Start Results (Cached Image)

| Run | Scheduled→Started | Image Pull Duration | Notes |
|-----|-------------------|---------------------|-------|
| 1 | 4s | 458 ms | Cache hit; negligible network transfer |
| 2 | 4s | 401 ms | Consistent with local cache read |

Warm improvement factors (vs cold averages):

- Startup latency reduction: ~7m09s → 4s (≈107× faster)
- Pull time reduction: ~350 s → <0.5 s (≈700× faster)

## 5. Root Cause & Bottlenecks

1. Large compressed image layers require sequential download + decompression on Windows node disks.
2. Throughput bounded by:
   - **ACR tier**: Standard tier shows variable throughput (20-34 MiB/s); Premium provides consistent higher throughput (37-39 MiB/s) with dedicated infrastructure.
   - Network bandwidth from ACR to node (regional proximity helps but tier matters significantly).
   - Node disk I/O (Windows container layer extraction tends to be slower than Linux).
3. Image composition (bundled large ISO / static assets) inflates cold pull times.
4. Scheduling and Extraction (estimated by the `Pulled` to `Created` duration) generates comparatively minor waits; main optimization target is image cold pull + ACR performance tier.

## 6. Technical Recommendations

### 6.1 Image Optimization

1. Use Windows Server 2022 base images (often smaller + performance improvements) if application compatible.
2. Remove large static assets (e.g., ISOs) from image layers; mount as volumes or download at runtime.
3. Use multi-stage builds to strip tooling, temp artifacts, and symbol files.
4. Deduplicate layers: ensure high-churn files are isolated in final layers to avoid the risk to invalidate large layers.

### 6.2 ACR & Network

- **Consider using an Azure Container Registry Premium SKU** to pull large and slow to cold start image(s), such as `compass`: testing demonstrates 36% faster pull times, 52% higher throughput, and 42% faster end-to-end startup vs ACR Standard tier. Premium provides dedicated infrastructure and consistent performance.
- Enable / Use ACR Private Endpoint to the AKS VNet for consistent (which requires ACR Premium also), low-latency private network path (reduces variability seen in Standard tier).
- Ensure ACR region matches AKS region for minimum network latency.

### 6.3 AKS Node & Operational Practices

- Evaluate and Tune the DaemonSet (Windows HostProcess) drafted in `.src/aks/prewarm-images-daemonset.yaml` that performs`crictl pull` (or `kubectl run` transient pod) of critical images immediately after node scale-out to convert ease cold starts into warm starts, if nodes are scaling a little ahead of workload scheduling.
- Monitor node disk performance (Managed Disk / Ephemeral OS disk); if constrained, move to faster SKU or larger disk for better I/O concurrency.

### 6.4 Cost-Performance Trade-offs

- **Premium ACR upgrade**: Adds ~$20/day vs Standard but delivers 36-42% startup improvement; ROI is immediate for time-sensitive workloads.
- Image slimming reduces storage and egress (if multi-region), indirectly lowering costs.

## 7. Summary

**Measured Results:**

- ACR Premium delivers 36% faster cold pull times (5m50s → 3m46s), 52% higher throughput (24.8 → 37.6 MiB/s), and 42% faster end-to-end startup (7m17s → 4m13s) compared to ACR Standard for large Windows container images (~8.13 GiB).
- Warm cache performance remains consistently fast at ~4 seconds regardless of tier.

**Key Findings:**

- Pull duration dominates overall startup latency (>80%)
- ACR Standard shows variable performance (20-34 MiB/s); Premium is consistent (37-39 MiB/s)
- Image decompression overhead on Windows (2.87 GB compressed → 9.91 GB inflated) adds significant extraction time
- Warm caching is extremely effective but requires proactive pre-warming strategy

**Short-term Optimization Levers (Ranked by Impact):**

1. **ACR Premium upgrade** (proven 36-42% improvement)
2. Image slimming (reduce image layers size, consider pulling assets from other sources instead of putting them in the image itself)
3. Proactive pre-warming via DaemonSet - will optimize the cold start into warm start for scaled nodes, if auto-scaling up node is tuned to "prepare" nodes ahead of workload scheduling
4. Governance around image composition

Executing the proposed plan should achieve sub-4-minute cold starts for a large Windows container image (~8.13 GiB) and maintain near-instant warm starts for production workloads.

---
Date: 2025-11-06
